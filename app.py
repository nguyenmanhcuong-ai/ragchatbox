import os
import re
from datetime import datetime
from typing import List, Dict, Optional
from dataclasses import dataclass
from pathlib import Path
from dotenv import load_dotenv
import streamlit as st
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.memory import ConversationBufferWindowMemory
from langchain.chains import ConversationalRetrievalChain
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader
from langchain.prompts import PromptTemplate
import google.generativeai as genai
import pypdf
import streamlit.components.v1 as components

import concurrent.futures
from typing import List, Optional, Dict, Any
from pathlib import Path
from datetime import datetime
import re
from functools import lru_cache
import numpy as np
from tqdm import tqdm
import torch
from sentence_transformers import SentenceTransformer
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import logging

# Ch·ªâ ƒë·ªãnh ƒë∆∞·ªùng d·∫´n t·ªõi file .env
BASE_DIR = Path(__file__).resolve().parent
env_path = BASE_DIR / ".env"
load_dotenv(dotenv_path=env_path)

# L·∫•y GOOGLE_API_KEY t·ª´ bi·∫øn m√¥i tr∆∞·ªùng
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
if not GOOGLE_API_KEY:
    st.error("GOOGLE_API_KEY kh√¥ng ƒë∆∞·ª£c t√¨m th·∫•y trong file .env.")
    raise ValueError("GOOGLE_API_KEY kh√¥ng ƒë∆∞·ª£c t√¨m th·∫•y trong file .env")

# Khai b√°o DIRS tr∆∞·ªõc khi s·ª≠ d·ª•ng
DIRS = {
    "cache": Path("cache"),
    "data": Path("data"),
    "history": Path("chat_history"),
    "models": Path("models")
}

# T·∫°o c√°c th∆∞ m·ª•c
for dir_path in DIRS.values():
    dir_path.mkdir(parents=True, exist_ok=True)

@dataclass
class DocumentMetadata:
    title: str
    file_type: str
    upload_date: datetime
    page_count: int
    file_path: str
    category: str
    effective_date: Optional[str] = None
    issuing_authority: Optional[str] = None
    document_number: Optional[str] = None
    document_year: Optional[int] = None


class DocumentProcessor:
    def __init__(self, batch_size: int = 20):
        self.batch_size = batch_size
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=2000,
            chunk_overlap=200,
            length_function=len,
            separators=[
                "\n\nƒêi·ªÅu", "\n\nCh∆∞∆°ng", "\n\nM·ª•c", "\n\nPh·∫ßn", 
                "\n\nPh·ª• l·ª•c", "\n\n", ".\n", "\n", ". ",
                ";\n", ";\\s", "ƒêi·ªÅu \\d+\\.", "Kho·∫£n \\d+\\.",
                "ƒêi·ªÅu \\d+\\.", "Kho·∫£n \\d+\\.", "ƒêi·ªÉm [a-z]\\)", 
                "CH∆Ø∆†NG [IVX]+", "M·ª•c \\d+", "Ph·∫ßn \\d+"
            ]
        )
        
        self.document_cache = {}
        self.chunk_cache = {}
        self.embedding_cache = {}
        self.text_cache = {}
        
        self._init_model()
        self._compile_patterns()

        
    def _init_model(self):
        try:
            self.model = SentenceTransformer('Cloyne/vietnamese-sbert-v3')
            self.model = self.model.to('cuda' if torch.cuda.is_available() else 'cpu')
            self.embeddings = HuggingFaceEmbeddings(model_name="Cloyne/vietnamese-sbert-v3")
        except Exception as e:
            logging.error(f"Model initialization error: {str(e)}")
            raise
            
    def _compile_patterns(self):
        """C·∫£i thi·ªán c√°c pattern ƒë·ªÉ nh·∫≠n di·ªán ch√≠nh x√°c h∆°n"""
        self.patterns = {
            'document_number': re.compile(r'S·ªë[: ]+(\d+/\d{4}/(?:Nƒê-CP|TT-BTC|Qƒê-TTg|UBTVQH|NQ-CP|Qƒê-BTC|VBPQ|CT|VP|CV|TB))'),
            'issue_date': re.compile(r'ng√†y\s*(\d{1,2}\s*th√°ng\s*\d{1,2}\s*nƒÉm\s*\d{4})|ng√†y\s*(\d{1,2}/\d{1,2}/\d{4})'),
            'effective_date': re.compile(r'c√≥\s*hi·ªáu\s*l·ª±c\s*(t·ª´|k·ªÉ t·ª´)\s*ng√†y\s*(\d{1,2}/\d{1,2}/\d{4})'),
            'authority': re.compile(r'(CH√çNH PH·ª¶|B·ªò[^,\n]*|·ª¶Y BAN[^,\n]*|QU·ªêC H·ªòI|TH·ª¶ T∆Ø·ªöNG[^,\n]*)'),
            # Pattern cho c·∫•u tr√∫c ph√°p lu·∫≠t
            'article': re.compile(r'ƒêi·ªÅu\s+(\d+[a-z]?)\.?\s*([^.]+)'),
            'clause': re.compile(r'(?:^|\n\s*)(\d+)\.(?:\s+|$)([^.]+)'),
            'point': re.compile(r'(?:^|\n\s*)([a-z])\)(?:\s+|$)([^;.]+)'),
            'sub_point': re.compile(r'(?:^|\n\s*)-\s+([^;.]+)')
        }

    def process_document(self, file) -> Optional[tuple[List, DocumentMetadata]]:
        try:
            file_id = hash(file.name)
            
            # Return cached results if available
            if file_id in self.document_cache:
                return self.document_cache[file_id]
            
            # Save and load document
            file_path = DIRS["upload"] / file.name
            with open(file_path, "wb") as f:
                f.write(file.getbuffer())
                
            loader = PyPDFLoader(str(file_path)) if file.name.endswith('.pdf') else Docx2txtLoader(str(file_path))
            documents = loader.load()
            
            # Clean documents
            with concurrent.futures.ThreadPoolExecutor() as executor:
                clean_futures = [executor.submit(self.clean_vietnamese_text, doc.page_content) 
                               for doc in documents]
                for doc, future in zip(documents, clean_futures):
                    doc.page_content = future.result()
            
            # Extract metadata once
            metadata = self._extract_document_metadata(documents)
            
            # Split into chunks once
            chunks = self.text_splitter.split_documents(documents)
            
            # In c√°c chunk ra terminal
            print("\n=== K·∫øt qu·∫£ chunk ===")
            for idx, chunk in enumerate(chunks):
                print(f"Chunk {idx+1}:")
                print(chunk.page_content[:500])  # In 500 k√Ω t·ª± ƒë·∫ßu ti√™n c·ªßa chunk (ho·∫∑c c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh theo y√™u c·∫ßu)
                print("----")

            
            # Process chunks with detailed metadata
            processed_chunks = self._process_chunks_batch(chunks, metadata)
            
            # Cache results
            result = (processed_chunks, metadata)
            self.document_cache[file_id] = result
            
            # Store chunks separately for reuse
            self.chunk_cache[file_id] = {
                chunk.metadata['chunk_id']: chunk for chunk in processed_chunks
            }
            
            return result
            
        except Exception as e:
            logging.error(f"Document processing error: {str(e)}")
            return None, None
            
    def _process_chunks_batch(self, chunks: List, metadata: DocumentMetadata) -> List:
        """
        Args:
            chunks: List of document chunks to process.
            metadata: Metadata for the entire document.

        Returns:
            A list of processed chunks with enriched metadata.
        """
        processed_chunks = []
        total_chunks = len(chunks)

        # Use tqdm for a progress bar
        with tqdm(total=total_chunks, desc="Processing chunks") as pbar:
            with ThreadPoolExecutor() as executor:
                # Submit tasks for processing each chunk
                futures = {
                    executor.submit(self._process_chunk, chunk, idx, metadata): idx
                    for idx, chunk in enumerate(chunks)
                }
                for future in as_completed(futures):
                    chunk_idx = futures[future]
                    try:
                        # Process and collect the result
                        result = future.result()
                        processed_chunks.append(result)
                        
                        # Update progress and print status
                        pbar.update(1)
                        print(f"Chunk {chunk_idx} processed successfully.")
                    except Exception as e:
                        logging.error(f"Error processing chunk {chunk_idx}: {str(e)}")
                        pbar.update(1)
        
        return processed_chunks

    def _process_chunk(self, text: Any, chunk_id: int, metadata: DocumentMetadata) -> Any:
        """
        Args:
            text: The content of the chunk to process.
            chunk_id: The unique identifier for the chunk.
            metadata: Metadata for the document.

        Returns:
            The processed chunk with updated metadata.
        """
        content = text.page_content

        # Extract legal references
        legal_refs = self._extract_legal_references(content)

        # Analyze penalties
        penalties = self._extract_penalties(content)

        # Determine section type
        section_type = self._detect_section_type(content)

        # Update metadata
        text.metadata.update({
            "chunk_id": chunk_id,
            "title": metadata.title,
            "document_type": metadata.category,
            "legal_references": legal_refs,
            "keywords": self._get_cached_keywords(content),
            "section_type": section_type,
            "penalties": penalties,
            "has_penalty": bool(penalties),
            "document_number": metadata.document_number,
            "document_year": metadata.document_year
        })

        # Print chunk details to the terminal
        print(f"Processing chunk {chunk_id}:")
        print(f"  Title: {metadata.title}")
        print(f"  Document Type: {metadata.category}")
        print(f"  Document Number: {metadata.document_number}")
        print(f"  Section Type: {section_type}")
        print(f"  Legal References: {legal_refs}")
        print(f"  Keywords: {self._get_cached_keywords(content)}")
        print(f"  Penalties: {penalties}")
        print(f"  Has Penalty: {bool(penalties)}")
        print("----------------------------------------------------")

        return text
        
    def get_cached_chunks(self, file_id: int) -> Optional[dict]:
        """Retrieve cached chunks for a file"""
        return self.chunk_cache.get(file_id)
        
    def clear_caches(self):
        """Clear all caches"""
        self.document_cache.clear()
        self.chunk_cache.clear()
        self.embedding_cache.clear()
        self.text_cache.clear()

    def _extract_document_metadata(self, documents: List) -> DocumentMetadata:
        """C·∫£i thi·ªán tr√≠ch xu·∫•t metadata"""
        first_doc = documents[0]
        content = first_doc.page_content[:2000]  # TƒÉng ph·∫°m vi qu√©t
        
        # Tr√≠ch xu·∫•t s·ªë vƒÉn b·∫£n v√† nƒÉm
        doc_match = re.search(self.patterns['document_number'], content)
        doc_num = None
        doc_year = None
        if doc_match:
            doc_num = doc_match.group(1)
            try:
                doc_year = int(doc_num.split('/')[1])
            except (IndexError, ValueError):
                pass

        # Tr√≠ch xu·∫•t ng√†y c√≥ hi·ªáu l·ª±c
        eff_date = None
        eff_match = re.search(self.patterns['effective_date'], content)
        if eff_match:
            eff_date = eff_match.group(2)

        # Tr√≠ch xu·∫•t c∆° quan ban h√†nh
        auth_match = re.search(self.patterns['authority'], content)
        authority = auth_match.group(1) if auth_match else None
        
        return DocumentMetadata(
            title=self._extract_title(documents),
            file_type=Path(first_doc.metadata.get('source', '')).suffix.lower()[1:],
            upload_date=datetime.now(),
            page_count=len(documents),
            file_path=first_doc.metadata.get('source', ''),
            category=self._detect_document_type(content),
            document_number=doc_num,
            document_year=doc_year,
            effective_date=eff_date,
            issuing_authority=authority
        )

    def _extract_title(self, documents: List) -> str:
        """Extract title with improved pattern matching"""
        if not documents:
            return ""
            
        content = documents[0].page_content[:1000]
        
        # Enhanced patterns for Vietnamese legal documents
        patterns = [
            r"^(?:LU·∫¨T|NGH·ªä ƒê·ªäNH|TH√îNG T∆Ø|QUY·∫æT ƒê·ªäNH|NGH·ªä QUY·∫æT)[\s\n:]+(.+?)(?=\n\n|\n(?:S·ªë|CƒÉn c·ª©|Theo|X√©t)|\Z)",
            r"^(?:V·ªÅ vi·ªác|V/v)[\s:]+(.+?)(?=\n|\Z)",
            r"^(.{10,150}?(?:lu·∫≠t|ngh·ªã ƒë·ªãnh|th√¥ng t∆∞|quy·∫øt ƒë·ªãnh).+?)(?=\n|\Z)"
        ]
        
        for pattern in patterns:
            if match := re.search(pattern, content, re.I | re.M | re.S):
                return re.sub(r'\s+', ' ', match.group(1)).strip()
                
        return re.sub(r'\s+', ' ', content.split('\n')[0])[:100]

    
    @lru_cache(maxsize=1000)
    def _get_cached_keywords(self, content: str) -> List[str]:
        """Cache k·∫øt qu·∫£ extraction keywords"""
        return self._extract_keywords(content)
        
    @lru_cache(maxsize=1000)
    def _get_cached_section_type(self, content: str) -> str:
        """Cache k·∫øt qu·∫£ detect section type"""
        return self._detect_section_type(content)

    def clean_vietnamese_text(self, text: str) -> str:
        """C·∫£i thi·ªán l√†m s·∫°ch vƒÉn b·∫£n ti·∫øng Vi·ªát"""
        if text in self.text_cache:
            return self.text_cache[text]
            
        # Chu·∫©n h√≥a kho·∫£ng tr·∫Øng
        text = re.sub(r'\s+', ' ', text)
        
        # Chu·∫©n h√≥a ƒë·ªãnh d·∫°ng ƒêi·ªÅu
        text = re.sub(r'ƒêi·ªÅu\s+(\d+)', r'ƒêi·ªÅu \1.', text)
        
        # Chu·∫©n h√≥a ƒë·ªãnh d·∫°ng Kho·∫£n
        text = re.sub(r'(?<=\n)(\d+)\s*\)', r'\1.', text)
        
        # Chu·∫©n h√≥a ƒë·ªãnh d·∫°ng ƒêi·ªÉm
        text = re.sub(r'(?<=\n)([a-z])\s*\)', r'\1)', text)
        
        # S·ª≠a l·ªói OCR ph·ªï bi·∫øn
        replacements = {
            r'ngu√≤i': 'ng∆∞·ªùi',
            r'Di·ªÅu': 'ƒêi·ªÅu',
            r'Chinh ph·ªß': 'Ch√≠nh ph·ªß',
            r'ngh·ªã ƒëinh': 'ngh·ªã ƒë·ªãnh',
            r'(?<=\d)\.(?=\d)': ',',  # S·ª≠a s·ªë th·∫≠p ph√¢n
        }
        
        for pattern, replacement in replacements.items():
            text = re.sub(pattern, replacement, text)
            
        result = text.strip()
        self.text_cache[text] = result
        return result


    def _detect_document_type(self, text: str) -> str:
        """Improved document type detection"""
        patterns = {
            'Lu·∫≠t': r'\b(?:LU·∫¨T|Lu·∫≠t)\b',
            'Ngh·ªã ƒë·ªãnh': r'\b(?:NGH·ªä ƒê·ªäNH|Ngh·ªã ƒë·ªãnh)\b',
            'Th√¥ng t∆∞': r'\b(?:TH√îNG T∆Ø|Th√¥ng t∆∞)\b',
            'Ngh·ªã quy·∫øt': r'\b(?:NGH·ªä QUY·∫æT|Ngh·ªã quy·∫øt)\b',
            'Quy·∫øt ƒë·ªãnh': r'\b(?:QUY·∫æT ƒê·ªäNH|Quy·∫øt ƒë·ªãnh)\b'
        }
        
        for doc_type, pattern in patterns.items():
            if re.search(pattern, text[:1000]):
                return doc_type
        return "Kh√°c"

    def _extract_legal_references(self, text: str) -> List[dict]:
        """Enhanced legal reference extraction"""
        references = []
        
        patterns = {
            'article': r'ƒêi·ªÅu\s+(\d+)[A-Za-z]?',
            'clause': r'Kho·∫£n\s+(\d+)',
            'point': r'[ƒê|d]i·ªÉm\s+([a-zA-Z])',
            'document_ref': r'(?:theo|t·∫°i|cƒÉn c·ª©)\s+([^.;]+(?:lu·∫≠t|ngh·ªã ƒë·ªãnh|th√¥ng t∆∞|quy·∫øt ƒë·ªãnh)[^.;]+)'
        }
        
        for ref_type, pattern in patterns.items():
            matches = re.finditer(pattern, text, re.I)
            for match in matches:
                references.append({
                    'type': ref_type,
                    'value': match.group(1),
                    'context': text[max(0, match.start()-50):match.end()+50]
                })
                
        return references

    def _extract_keywords(self, text: str) -> List[str]:
        """C·∫£i thi·ªán tr√≠ch xu·∫•t t·ª´ kh√≥a ph√°p l√Ω"""
        legal_terms = [
            r'x·ª≠\s+ph·∫°t', r'vi\s+ph·∫°m', r'b·∫Øt\s+bu·ªôc', 
            r'quy·ªÅn', r'nghƒ©a\s+v·ª•', r'tr√°ch\s+nhi·ªám',
            r'th·∫©m\s+quy·ªÅn', r'gi·∫•y\s+ph√©p', r'ch·ª©ng\s+nh·∫≠n',
            r'c·∫•m', r'ƒë√¨nh\s+ch·ªâ', r't∆∞·ªõc\s+quy·ªÅn',
            r't·∫°m\s+ƒë√¨nh\s+ch·ªâ', r'thu\s+h·ªìi', r'ki·ªÉm\s+tra',
            r'thanh\s+tra', r'khi·∫øu\s+n·∫°i', r't·ªë\s+c√°o'
        ]
        
        keywords = []
        for term in legal_terms:
            if re.search(rf'\b{term}\b', text, re.I):
                # L·∫•y ng·ªØ c·∫£nh xung quanh t·ª´ kh√≥a
                matches = re.finditer(rf'\b{term}\b', text, re.I)
                for match in matches:
                    context = text[max(0, match.start()-30):min(len(text), match.end()+30)]
                    keywords.append({
                        'term': match.group(0),
                        'context': context.strip(),
                        'position': match.start()
                    })
                
        return keywords


    def _extract_penalties(self, text: str) -> List[dict]:
        """Tr√≠ch xu·∫•t th√¥ng tin v·ªÅ h√¨nh ph·∫°t"""
        penalties = []
        
        # Pattern cho h√¨nh ph·∫°t ti·ªÅn
        money_patterns = [
            r'(?:ph·∫°t\s+ti·ªÅn|m·ª©c\s+ph·∫°t)\s*(?:t·ª´\s*)?(\d+(?:\.\d+)?)\s*(?:ƒë·∫øn\s*)?(\d+(?:\.\d+)?)?\s*(?:tri·ªáu\s*)?(?:ƒë·ªìng|VNƒê)',
            r'(?:ph·∫°t|n·ªôp)\s*(?:t·ª´\s*)?(\d+(?:\.\d+)?)\s*%\s*(?:ƒë·∫øn\s*)?(\d+(?:\.\d+)?)?\s*%\s*(?:c·ªßa|tr√™n|trong)?',
        ]
        
        # Pattern cho h√¨nh ph·∫°t kh√°c (bao g·ªìm tr·ª´ ƒëi·ªÉm GPLX)
        other_patterns = [
            r'(ƒë√¨nh\s*ch·ªâ)\s*(?:ho·∫°t\s*ƒë·ªông)?\s*(?:trong\s*th·ªùi\s*h·∫°n|th·ªùi\s*h·∫°n)?\s*(\d+)\s*(?:th√°ng|nƒÉm)',
            r'(t∆∞·ªõc\s*quy·ªÅn)\s*(?:s·ª≠\s*d·ª•ng)?\s*([^.]+?)\s*(?:trong\s*th·ªùi\s*h·∫°n|th·ªùi\s*h·∫°n)?\s*(\d+)\s*(?:th√°ng|nƒÉm)',
            r'(t·ªãch\s*thu)\s*([^.]+)',
            r'(bu·ªôc)\s*([^.]+)',
            r'(thu\s*h·ªìi)\s*([^.]+)',
            # Th√™m pattern cho tr·ª´ ƒëi·ªÉm GPLX
            r'(tr·ª´)\s*(\d+)\s*(?:ƒëi·ªÉm)?\s*(?:tr√™n|trong|v√†o)?\s*(?:gi·∫•y\s*ph√©p\s*l√°i\s*xe|GPLX|b·∫±ng\s*l√°i)',
        ]
        
        # Tr√≠ch xu·∫•t h√¨nh ph·∫°t ti·ªÅn
        for pattern in money_patterns:
            for match in re.finditer(pattern, text, re.I):
                penalties.append({
                    'type': 'monetary',
                    'amount': match.group(1),
                    'context': match.group(0),
                    'full_text': text[max(0, match.start()-50):match.end()+50]
                })
        
        # Tr√≠ch xu·∫•t h√¨nh ph·∫°t kh√°c (bao g·ªìm tr·ª´ ƒëi·ªÉm GPLX)
        for pattern in other_patterns:
            for match in re.finditer(pattern, text, re.I):
                penalties.append({
                    'type': 'administrative',
                    'action': match.group(1),
                    'detail': match.group(2) if len(match.groups()) > 1 else None,
                    'context': match.group(0),
                    'full_text': text[max(0, match.start()-50):match.end()+50]
                })
        
        return penalties

    def _detect_section_type(self, text: str) -> str:
        """C·∫£i thi·ªán ph√°t hi·ªán lo·∫°i m·ª•c"""
        type_patterns = {
            "Ch·∫ø t√†i": r'x·ª≠\s+ph·∫°t|vi\s+ph·∫°m|ch·∫ø\s+t√†i|tr√°ch\s+nhi·ªám\s+(?:h√¨nh\s+s·ª±|h√†nh\s+ch√≠nh)|bi·ªán\s+ph√°p\s+(?:x·ª≠\s+l√Ω|ngƒÉn\s+ch·∫∑n)',
            "ƒê·ªãnh nghƒ©a": r'ƒë·ªãnh\s+nghƒ©a|gi·∫£i\s+th√≠ch|kh√°i\s+ni·ªám|thu·∫≠t\s+ng·ªØ|quy\s+ƒë·ªãnh\s+chung',
            "Ph·∫°m vi": r'ph·∫°m\s+vi|ƒë·ªëi\s+t∆∞·ª£ng\s+(?:√°p\s+d·ª•ng|ƒëi·ªÅu\s+ch·ªânh)|kh√¥ng\s+√°p\s+d·ª•ng',
            "Quy tr√¨nh": r'tr√¨nh\s+t·ª±|th·ªß\s+t·ª•c|c√°c\s+b∆∞·ªõc|quy\s+tr√¨nh|ph∆∞∆°ng\s+th·ª©c|c√°ch\s+th·ª©c',
            "T·ªï ch·ª©c th·ª±c hi·ªán": r't·ªï\s+ch·ª©c\s+th·ª±c\s+hi·ªán|ƒëi·ªÅu\s+kho·∫£n\s+thi\s+h√†nh|hi·ªáu\s+l·ª±c\s+thi\s+h√†nh',
            "Quy·ªÅn v√† nghƒ©a v·ª•": r'quy·ªÅn|nghƒ©a\s+v·ª•|tr√°ch\s+nhi·ªám|nghƒ©a\s+v·ª•\s+c·ªßa|quy·ªÅn\s+c·ªßa',
            "ƒêi·ªÅu kho·∫£n chuy·ªÉn ti·∫øp": r'chuy·ªÉn\s+ti·∫øp|ƒëi·ªÅu\s+kho·∫£n\s+chuy·ªÉn\s+ti·∫øp|quy\s+ƒë·ªãnh\s+chuy·ªÉn\s+ti·∫øp',
            "Th·∫©m quy·ªÅn": r'th·∫©m\s+quy·ªÅn|c√≥\s+quy·ªÅn|ƒë∆∞·ª£c\s+quy·ªÅn|c√≥\s+tr√°ch\s+nhi·ªám',
            "Gi·∫•y ph√©p": r'gi·∫•y\s+ph√©p|gi·∫•y\s+ch·ª©ng\s+nh·∫≠n|c·∫•p\s+ph√©p|ƒëƒÉng\s+k√Ω',
            "Thanh tra ki·ªÉm tra": r'thanh\s+tra|ki·ªÉm\s+tra|gi√°m\s+s√°t|ki·ªÉm\s+so√°t'

        }
        
        for section_type, pattern in type_patterns.items():
            if re.search(pattern, text, re.I):
                return section_type
                
        return "N·ªôi dung chung"

class TrafficLawAssistant:
    """
    Tr·ª£ l√Ω x·ª≠ l√Ω c√°c c√¢u h·ªèi li√™n quan ƒë·∫øn Lu·∫≠t Giao th√¥ng Vi·ªát Nam.
    S·ª≠ d·ª•ng m√¥ h√¨nh Generative AI c·ªßa Google ƒë·ªÉ tr·∫£ l·ªùi.
    """

    def __init__(self):
        """Kh·ªüi t·∫°o tr·ª£ l√Ω v·ªõi m√¥ h√¨nh LLM v√† b·ªô nh·ªõ cu·ªôc tr√≤ chuy·ªán."""
        self.llm = ChatGoogleGenerativeAI(
            model="gemini-1.5-pro",
            google_api_key=GOOGLE_API_KEY,
            temperature=0.3,
            top_p=0.85,
            streaming=True,
            max_tokens=1024,
            system_prompt="B·∫°n l√† tr·ª£ l√Ω th√¢n thi·ªán v√† am hi·ªÉu v·ªÅ Lu·∫≠t Giao th√¥ng ƒë∆∞·ªùng b·ªô Vi·ªát Nam."
        )
        
        self.memory = ConversationBufferWindowMemory(
            memory_key="chat_history",
            output_key="answer",
            return_messages=True,
            k=3
        )

    def _create_prompt_template(self) -> PromptTemplate:
        """
        T·∫°o m·∫´u prompt c·∫•u tr√∫c cho c√°c c√¢u h·ªèi li√™n quan ƒë·∫øn Lu·∫≠t Giao th√¥ng.
        
        Returns:
            PromptTemplate: M·∫´u prompt ƒë∆∞·ª£c c·∫•u h√¨nh
        """
        template = """
        D·ª±a v√†o c√°c t√†i li·ªáu sau:
        {context}

        L·ªãch s·ª≠ trao ƒë·ªïi:
        {chat_history}

        C√¢u h·ªèi: {question}

        H√£y tr·∫£ l·ªùi theo format sau:
                
        
        **PH√ÇN T√çCH T√åNH HU·ªêNG**
        [M√¥ t·∫£ ng·∫Øn g·ªçn v·ªÅ t√¨nh hu·ªëng v√† v·∫•n ƒë·ªÅ ch√≠nh]
        

        **QUY ƒê·ªäNH PH√ÅP LU·∫¨T LI√äN QUAN**
        [Ch·ªâ r√µ ngh·ªã ƒë·ªãnh, ƒëi·ªÅu, kho·∫£n, ƒëi·ªÉm √°p d·ª•ng c√πng gi·∫£i th√≠ch ng·∫Øn g·ªçn.]


        **KHUY·∫æN NGH·ªä AN TO√ÄN**
       [C√°c khuy·∫øn ngh·ªã an to√†n]

        ‚ö†Ô∏è **L∆ØU √ù QUAN TR·ªåNG**
        [Nh·ªØng ƒëi·ªÉm c·∫ßn ƒë·∫∑c bi·ªát ch√∫ √Ω]
        """
        
        return PromptTemplate(
            input_variables=["context", "chat_history", "question"],
            template=template.strip()
        )
        

    def setup_chain(self, vectorstore):
        """
        Thi·∫øt l·∫≠p chu·ªói cu·ªôc tr√≤ chuy·ªán v·ªõi t√≠ch h·ª£p kho d·ªØ li·ªáu vector.
        
        Args:
            vectorstore: Kho d·ªØ li·ªáu vector ƒë·ªÉ truy v·∫•n t√†i li·ªáu
            
        Returns:
            ConversationalRetrievalChain: Chu·ªói cu·ªôc tr√≤ chuy·ªán ƒë∆∞·ª£c c·∫•u h√¨nh
        """
        chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=vectorstore.as_retriever(
                search_type="similarity",
                search_kwargs={"k": 3}
            ),
            memory=self.memory,
            combine_docs_chain_kwargs={
                "prompt": self._create_prompt_template()
            },
            return_source_documents=True,
            verbose=False
        )
        return chain

    def _format_section_content(self, content: str) -> str:
        """
        ƒê·ªãnh d·∫°ng n·ªôi dung c·ªßa t·ª´ng ph·∫ßn v·ªõi d√≤ng xu·ªëng ƒë√∫ng c√°ch.
        
        Args:
            content (str): N·ªôi dung vƒÉn b·∫£n c·∫ßn ƒë·ªãnh d·∫°ng
            
        Returns:
            str: N·ªôi dung ƒë∆∞·ª£c ƒë·ªãnh d·∫°ng v·ªõi d√≤ng xu·ªëng ph√π h·ª£p
        """
        # T√°ch vƒÉn b·∫£n th√†nh c√°c c√¢u
        sentences = re.split(r'(?<=[.!?])\s+', content)
        # Lo·∫°i b·ªè c√°c c√¢u tr·ªëng v√† gh√©p l·∫°i v·ªõi d√≤ng xu·ªëng
        return '\n'.join(sent.strip() for sent in sentences if sent.strip())

    def enhance_response(self, response: str) -> str:
        """
        T·ªëi ∆∞u h√≥a ƒë·ªãnh d·∫°ng c√¢u tr·∫£ l·ªùi v·ªõi phong c√°ch chuy√™n nghi·ªáp, x·ª≠ l√Ω ch√≠nh x√°c k√Ω t·ª± **.

        Args:
            response (str): VƒÉn b·∫£n c√¢u tr·∫£ l·ªùi th√¥
            
        Returns:
            str: C√¢u tr·∫£ l·ªùi ƒë∆∞·ª£c ƒë·ªãnh d·∫°ng v√† t·ªëi ∆∞u h√≥a
        """
        # C√°c ti√™u ƒë·ªÅ ph·∫ßn
        sections = [
            'PH√ÇN T√çCH T√åNH HU·ªêNG',
            'QUY ƒê·ªäNH PH√ÅP LU·∫¨T LI√äN QUAN',
            'KHUY·∫æN NGH·ªä AN TO√ÄN',
            'L∆ØU √ù QUAN TR·ªåNG'
        ]

        # Chu·∫©n h√≥a to√†n b·ªô vƒÉn b·∫£n tr∆∞·ªõc khi x·ª≠ l√Ω
        enhanced = self._normalize_bold_markers(response)

        # ƒê·ªãnh d·∫°ng ti√™u ƒë·ªÅ ph·∫ßn
        for section in sections:
            pattern = rf"{section}:"
            replacement = rf"\n\n{section}\n"
            enhanced = enhanced.replace(pattern, replacement)

        # T√°ch n·ªôi dung th√†nh c√°c ph·∫ßn
        sections_content = {}
        current_section = None
        lines = enhanced.split('\n')
        current_content = []

        for line in lines:
            if any(section in line for section in sections):
                if current_section and current_content:
                    sections_content[current_section] = '\n'.join(current_content)
                current_section = line.strip() + ': '
                current_content = []
            elif line.strip():
                current_content.append(line)

        if current_section and current_content:
            sections_content[current_section] = '\n'.join(current_content)

        # ƒê·ªãnh d·∫°ng n·ªôi dung t·ª´ng ph·∫ßn
        formatted_sections = []
        for section, content in sections_content.items():
            # ƒê·ªãnh d·∫°ng bullet point
            if '‚Ä¢' in content:
                bullet_points = content.split('‚Ä¢')
                formatted_bullets = []
                for point in bullet_points:
                    if point.strip():
                        formatted_point = self._format_section_content(point.strip())
                        formatted_bullets.append(f"‚Ä¢ {formatted_point}")
                formatted_content = '\n'.join(formatted_bullets)
            else:
                formatted_content = self._format_section_content(content)

            formatted_sections.append(f"{section}\n{formatted_content}")

        # K·∫øt h·ª£p c√°c ph·∫ßn tr·ªü l·∫°i
        enhanced = '\n\n'.join(formatted_sections)

        # √Åp d·ª•ng quy t·∫Øc ƒë·ªãnh d·∫°ng b·ªï sung
        formatting_rules = [
            # ƒê·ªãnh d·∫°ng danh s√°ch s·ªë th·ª© t·ª± v·ªõi kho·∫£ng c√°ch th√≠ch h·ª£p
            (r'^(\d+)\.\s', lambda m: f"{m.group(1)}. ", re.MULTILINE),
            
            # L√†m n·ªïi b·∫≠t c√°c tr√≠ch d·∫´n ƒëi·ªÅu lu·∫≠t
            (r'(ƒêi·ªÅu \d+[^()]*?(?:, Kho·∫£n \d+[^()]*?)?(?:, ƒêi·ªÉm [a-z][^()]*?)?\))', r'<strong>\1</strong>', 0),
            
            # L√†m n·ªïi b·∫≠t ghi ch√∫ quan tr·ªçng
            (r'(!.*?[.!?])', r'<strong>\1</strong>', 0),
            
            # L√†m s·∫°ch kh√¥ng gian th·ª´a
            (r'\n{3,}', '\n\n', 0)
        ]

        # √Åp d·ª•ng t·ª´ng quy t·∫Øc ƒë·ªãnh d·∫°ng
        for pattern, replacement, flags in formatting_rules:
            enhanced = re.sub(pattern, replacement, enhanced, flags=flags)

        return enhanced.strip()

    def _normalize_bold_markers(self, text: str) -> str:
        """
        Chu·∫©n h√≥a k√Ω t·ª± ** trong vƒÉn b·∫£n, x√≥a t·∫•t c·∫£ ** kh√¥ng n·∫±m trong c·∫∑p h·ª£p l·ªá v√† chuy·ªÉn th√†nh HTML.

        Args:
            text (str): VƒÉn b·∫£n c·∫ßn chu·∫©n h√≥a
            
        Returns:
            str: VƒÉn b·∫£n ƒë√£ ƒë∆∞·ª£c chu·∫©n h√≥a v·ªõi <strong> thay v√¨ **
        """
        result = []
        i = 0
        length = len(text)

        while i < length:
            if i + 1 < length and text[i:i+2] == '**':
                # T√¨m v·ªã tr√≠ ƒë√≥ng c·ªßa c·∫∑p **
                start = i
                i += 2
                found_close = False
                content = []
                
                while i < length:
                    if i + 1 < length and text[i:i+2] == '**':
                        found_close = True
                        i += 2
                        break
                    content.append(text[i])
                    i += 1
                
                content_str = ''.join(content)
                # Ch·ªâ gi·ªØ ƒë·ªãnh d·∫°ng <strong> n·∫øu n·ªôi dung l√† ƒëi·ªÅu lu·∫≠t, ghi ch√∫, ho·∫∑c ti√™u ƒë·ªÅ
                if found_close and content and (
                    content_str.startswith('ƒêi·ªÅu ') or 
                    content_str.startswith('!') or 
                    any(section in content_str for section in [
                        'PH√ÇN T√çCH T√åNH HU·ªêNG', 
                        'QUY ƒê·ªäNH PH√ÅP LU·∫¨T LI√äN QUAN', 
                        'KHUY·∫æN NGH·ªä AN TO√ÄN', 
                        'L∆ØU √ù QUAN TR·ªåNG'
                    ])
                ):
                    result.append(f"<strong>{content_str}</strong>")
                else:
                    result.append(content_str)
            else:
                result.append(text[i])
                i += 1
        
        return ''.join(result)

    def get_response(self, question: str, vectorstore) -> str:
        """
        L·∫•y c√¢u tr·∫£ l·ªùi t·ª´ tr·ª£ l√Ω v√† t·ªëi ∆∞u h√≥a ƒë·ªãnh d·∫°ng.
        
        Args:
            question (str): C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
            vectorstore: Kho d·ªØ li·ªáu vector ƒë·ªÉ truy v·∫•n t√†i li·ªáu
            
        Returns:
            str: C√¢u tr·∫£ l·ªùi ƒë√£ ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a ƒë·ªãnh d·∫°ng
        """
        chain = self.setup_chain(vectorstore)
        result = chain.run(question)
        enhanced_response = self.enhance_response(result['answer'])
        return enhanced_response


class DocumentManager:
    def __init__(self, doc_processor):
        self.doc_processor = doc_processor
        if 'processed_files' not in st.session_state:
            st.session_state.processed_files = {}
        if 'failed_files' not in st.session_state:
            st.session_state.failed_files = []
            
    def load_files_from_data_directory(self) -> bool:
        """
        Load and process all files from the data directory automatically
        """
        success = False
        data_dir = Path("data")
        
        # Create data directory if it doesn't exist
        data_dir.mkdir(parents=True, exist_ok=True)
        
        # Get all PDF and DOCX files from data directory
        files = list(data_dir.glob("*.pdf")) + list(data_dir.glob("*.docx"))
        
        if not files:
            st.warning("üìÇ Kh√¥ng t√¨m th·∫•y t√†i li·ªáu trong th∆∞ m·ª•c data. Vui l√≤ng th√™m file PDF ho·∫∑c DOCX v√†o th∆∞ m·ª•c.")
            return False
            
        for file_path in files:
            try:
                with st.status(f"üìÑ ƒêang x·ª≠ l√Ω: {file_path.name}") as status:
                    file_id = hash(file_path.name)
                    
                    if file_id in st.session_state.processed_files:
                        status.info(f"‚ÑπÔ∏è S·ª≠ d·ª•ng d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω: {file_path.name}")
                        continue
                    
                    # Process document directly from file path
                    texts, metadata = self.process_file(file_path)
                    
                    if texts and metadata:
                        st.session_state.processed_files[file_id] = {
                            'texts': texts,
                            'metadata': metadata,
                            'name': file_path.name
                        }
                        status.success(f"‚úÖ X·ª≠ l√Ω th√†nh c√¥ng: {file_path.name}")
                        success = True
                    else:
                        st.session_state.failed_files.append(file_path.name)
                        status.error(f"‚ùå Kh√¥ng th·ªÉ x·ª≠ l√Ω: {file_path.name}")
                
            except Exception as e:
                st.session_state.failed_files.append(file_path.name)
                st.error(f"‚ùå L·ªói khi x·ª≠ l√Ω {file_path.name}: {str(e)}")
        
        if st.session_state.processed_files:
            st.success(f"""
            üìä T·ªïng k·∫øt x·ª≠ l√Ω:
            - S·ªë file ƒë√£ x·ª≠ l√Ω th√†nh c√¥ng: {len(st.session_state.processed_files)}
            - S·ªë file th·∫•t b·∫°i: {len(st.session_state.failed_files)}
            """)
            
        return success and len(st.session_state.processed_files) > 0
    
    def process_file(self, file_path: Path) -> tuple:
        """Process a single file from disk"""
        try:
            # Determine loader based on file extension
            if file_path.suffix.lower() == '.pdf':
                loader = PyPDFLoader(str(file_path))
            elif file_path.suffix.lower() == '.docx':
                loader = Docx2txtLoader(str(file_path))
            else:
                raise ValueError(f"Unsupported file type: {file_path.suffix}")
                
            documents = loader.load()
            
            # Clean documents
            with concurrent.futures.ThreadPoolExecutor() as executor:
                clean_futures = [executor.submit(self.doc_processor.clean_vietnamese_text, doc.page_content) 
                               for doc in documents]
                for doc, future in zip(documents, clean_futures):
                    doc.page_content = future.result()
            
            # Extract metadata
            metadata = self.doc_processor._extract_document_metadata(documents)
            
            # Split into chunks
            chunks = self.doc_processor.text_splitter.split_documents(documents)
            
            # Process chunks with detailed metadata
            processed_chunks = self.doc_processor._process_chunks_batch(chunks, metadata)
            
            return processed_chunks, metadata
            
        except Exception as e:
            logging.error(f"Error processing file {file_path}: {str(e)}")
            return None, None

    
    def get_processed_data(self):
        all_texts = []
        all_metadata = []
        
        for file_data in st.session_state.processed_files.values():
            all_texts.extend(file_data['texts'])
            all_metadata.append(file_data['metadata'])
            
        return all_texts, all_metadata
    
    def clear_data(self):
        st.session_state.processed_files = {}
        st.session_state.failed_files = []
        self.doc_processor.chunk_cache.clear()
        self.doc_processor.embedding_cache.clear()

def setup_suggested_questions():
    """Setup suggested questions for the chat interface"""
    return [
        {
            "category": "Vi ph·∫°m & X·ª≠ ph·∫°t",
            "questions": [
                "L·ªói √¥ t√¥ v∆∞·ª£t ƒë√®n ƒë·ªè b·ªã ph·∫°t bao nhi√™u ti·ªÅn?",
                "ƒêi·ªÅu khi·ªÉn xe m√°y kh√¥ng c√≥ b·∫±ng l√°i b·ªã ph·∫°t nh∆∞ th·∫ø n√†o?",
                "C√°c m·ª©c x·ª≠ ph·∫°t khi l√°i xe m√°y c√≥ n·ªìng ƒë·ªô c·ªìn?",
                "M·ª©c ph·∫°t khi kh√¥ng ƒë·ªôi m≈© b·∫£o hi·ªÉm?",
                "Xe m√°y ƒëi qu√° t·ªëc ƒë·ªô x·ª≠ ph·∫°t th·∫ø n√†o?"
            ]
        },
        {
            "category": "Gi·∫•y t·ªù & Th·ªß t·ª•c",
            "questions": [
                "Th·ªß t·ª•c ƒëƒÉng k√Ω xe m√°y m·ªõi c·∫ßn nh·ªØng g√¨?",
                "H·ªì s∆° thi b·∫±ng l√°i xe m√°y g·ªìm nh·ªØng g√¨?",
                "Th·ªùi h·∫°n ƒëƒÉng ki·ªÉm xe √¥ t√¥ l√† bao l√¢u?",
                "C√°ch tra c·ª©u ph·∫°t ngu·ªôi online?",
                "Th·ªß t·ª•c sang t√™n xe m√°y kh√°c t·ªânh?"
            ]
        },
        {
            "category": "Quy ƒë·ªãnh & ƒêi·ªÅu lu·∫≠t",
            "questions": [
                "Quy ƒë·ªãnh v·ªÅ ƒë·ªô tu·ªïi ƒë∆∞·ª£c ƒëi·ªÅu khi·ªÉn xe m√°y?",
                "C√°c tr∆∞·ªùng h·ª£p b·ªã t∆∞·ªõc b·∫±ng l√°i xe?",
                "Quy ƒë·ªãnh v·ªÅ t·ªëc ƒë·ªô trong khu d√¢n c∆∞?",
                "ƒêi·ªÅu ki·ªán ƒë·ªÉ ƒë∆∞·ª£c c·∫•p b·∫±ng l√°i xe?",
                "Quy ƒë·ªãnh v·ªÅ s·ªë ng∆∞·ªùi ch·ªü tr√™n xe m√°y?"
            ]
        }
    ]

def render_search_interface():
    """Render a collapsible search interface with suggestions"""
    suggested_questions = setup_suggested_questions()
    
    # Custom Component for Search Interface
    search_interface_html = """
    <style>
        .search-panel {
            background: white;
            border-radius: 12px;
            box-shadow: 0 2px 12px rgba(0, 0, 0, 0.1);
            margin: 20px 0;
            overflow: hidden;
        }
        
        .search-header {
            padding: 16px;
            background: #f8fafc;
            border-bottom: 1px solid #e2e8f0;
            font-weight: 600;
            color: #1e40af;
            display: flex;
            align-items: center;
            gap: 8px;
            cursor: pointer;
        }
        
        .questions-container {
            padding: 12px;
            max-height: 500px;
            overflow-y: auto;
        }
        
        .category-accordion {
            margin-bottom: 8px;
            border: 1px solid #e2e8f0;
            border-radius: 8px;
            overflow: hidden;
        }
        
        .category-header {
            padding: 12px 16px;
            background: #f8fafc;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: space-between;
            font-weight: 500;
            color: #475569;
        }
        
        .category-header:hover {
            background: #f1f5f9;
        }
        
        .category-content {
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.3s ease-out;
            background: white;
        }
        
        .category-content.active {
            max-height: 500px;
            padding: 12px;
        }
        
        .question-item {
            padding: 10px 16px;
            margin: 4px 0;
            border-radius: 6px;
            cursor: pointer;
            color: #0369a1;
            display: flex;
            align-items: center;
            gap: 8px;
            transition: all 0.2s ease;
        }
        
        .question-item:hover {
            background: #f0f9ff;
        }
        
        .question-item:active {
            background: #e0f2fe;
        }
        
        .toggle-icon {
            transition: transform 0.3s ease;
        }
        
        .category-header.active .toggle-icon {
            transform: rotate(180deg);
        }
        
        /* Scrollbar styling */
        .questions-container::-webkit-scrollbar {
            width: 6px;
        }
        
        .questions-container::-webkit-scrollbar-track {
            background: #f1f5f9;
        }
        
        .questions-container::-webkit-scrollbar-thumb {
            background: #cbd5e1;
            border-radius: 3px;
        }
    </style>
    
    <div class="search-panel">
        <div class="search-header" id="mainToggle">
            <span>üí°</span> C√¢u h·ªèi th∆∞·ªùng g·∫∑p theo ch·ªß ƒë·ªÅ
            <span class="toggle-icon">‚ñº</span>
        </div>
        <div class="questions-container" id="questionCategories">
        </div>
    </div>
    """
    
    # Convert suggested questions to JavaScript
    questions_js = f"const suggestedQuestions = {str(suggested_questions).replace('True', 'true').replace('False', 'false')};"
    
    components.html(
        search_interface_html + 
        f"""
        <script>
        {questions_js}
        
        function triggerChatSubmit(question) {{
            // T√¨m input field c·ªßa chat
            const chatInputs = window.parent.document.querySelectorAll('textarea');
            const chatInput = Array.from(chatInputs).find(input => 
                input.placeholder && input.placeholder.includes('H·ªèi v·ªÅ n·ªôi dung')
            );
            
            if (chatInput) {{
                // Set gi√° tr·ªã cho input
                const nativeTextAreaValue = Object.getOwnPropertyDescriptor(HTMLTextAreaElement.prototype, "value");
                const nativeInputValue = Object.getOwnPropertyDescriptor(HTMLInputElement.prototype, "value");
                
                function setNativeValue(element, value) {{
                    const valueSetter = Object.getOwnPropertyDescriptor(element, 'value')?.set 
                        || nativeTextAreaValue?.set 
                        || nativeInputValue?.set;
                        
                    if (valueSetter) {{
                        const prototype = Object.getPrototypeOf(element);
                        const prototypeValueSetter = Object.getOwnPropertyDescriptor(prototype, 'value')?.set;
                        
                        if (valueSetter !== prototypeValueSetter) {{
                            prototypeValueSetter.call(element, value);
                        }} else {{
                            valueSetter.call(element, value);
                        }}
                    }}
                }}
                
                setNativeValue(chatInput, question);
                chatInput.dispatchEvent(new Event('input', {{ bubbles: true }}));
                
                // T√¨m v√† click n√∫t submit
                const buttons = window.parent.document.querySelectorAll('button');
                const sendButton = Array.from(buttons).find(button => 
                    button.innerHTML.includes('‚Üµ')
                );
                
                if (sendButton) {{
                    sendButton.click();
                }}
            }}
        }}

        function createQuestionElement(question) {{
            const div = document.createElement('div');
            div.className = 'question-item';
            div.innerHTML = `<span>‚ùì</span>${{question}}`;
            
            div.onclick = (event) => {{
                event.stopPropagation();
                div.style.background = '#bae6fd';
                setTimeout(() => {{
                    div.style.background = '';
                }}, 200);
                
                triggerChatSubmit(question);
            }};
            
            return div;
        }}
        
        // Render categories function
        function renderCategories() {{
            const categoriesContainer = document.getElementById('questionCategories');
            
            suggestedQuestions.forEach(category => {{
                const categoryDiv = document.createElement('div');
                categoryDiv.className = 'category-accordion';
                
                const header = document.createElement('div');
                header.className = 'category-header';
                header.innerHTML = `
                    ${{category.category}}
                    <span class="toggle-icon">‚ñº</span>
                `;
                
                const content = document.createElement('div');
                content.className = 'category-content';
                
                header.onclick = (event) => {{
                    event.stopPropagation();
                    header.classList.toggle('active');
                    content.classList.toggle('active');
                }};
                
                category.questions.forEach(question => {{
                    content.appendChild(createQuestionElement(question));
                }});
                
                categoryDiv.appendChild(header);
                categoryDiv.appendChild(content);
                categoriesContainer.appendChild(categoryDiv);
            }});
        }}
        
        // Initial render
        renderCategories();
        </script>
        """,
        height=600
    )
   
# Must be the first Streamlit command
st.set_page_config(
    page_title="Tr·ª£ L√Ω Ph√°p lu·∫≠t",
    page_icon="üö¶",
    layout="wide",
    initial_sidebar_state="expanded"
)

def setup_styles():
    st.markdown("""
        <style>
            /* Base styles */
            @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap');
            
            * {
                font-family: 'Inter', sans-serif;
            }
            
            /* Header styling */
            .header {
                background: linear-gradient(135deg, #1e40af 0%, #3b82f6 100%);
                padding: 1.5rem;
                border-radius: 12px;
                margin-bottom: 2rem;
                box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);
            }
            
            .app-title {
                color: white;
                font-size: 2.5rem;
                font-weight: 700;
                text-align: center;
                margin-bottom: 0.5rem;
            }
            
            .app-subtitle {
                color: rgba(255, 255, 255, 0.9);
                text-align: center;
                font-size: 1.1rem;
            }
            
            /* Chat interface */
            .chat-container {
                border-radius: 12px;
                background: #f8fafc;
                padding: 1rem;
                margin-bottom: 1rem;
            }
            
            .chat-message {
                padding: 1rem;
                margin-top: -11px;
                border-radius: 8px;
                box-shadow: 0 2px 4px rgba(0, 0, 0, 0.05);
            }
             
            
            .assistant-message {
                background: white;
                margin-right: 2rem;
            }
            
            /* Chat history styling */
            .chat-history-button {
                background: transparent;
                border: none;
                padding: 0.75rem;
                border-radius: 8px;
                text-align: left;
                width: 100%;
                transition: all 0.2s ease;
                color: #1e293b;
                font-size: 0.95rem;
            }
            
            .chat-history-button:hover {
                background: #f1f5f9;
            }
            
            .new-chat-button {
                background: #3b82f6 !important;
                color: white !important;
                padding: 0.75rem !important;
                border-radius: 8px !important;
                font-weight: 500 !important;
                margin-bottom: 1rem !important;
                text-align: center !important;
            }
            
            .new-chat-button:hover {
                background: #2563eb !important;
            }
            
            /* Source citations */
            .source-citation {
                background: #f1f5f9;
                padding: 0.5rem;
                border-radius: 6px;
                margin-top: 0.5rem;
                font-size: 0.9rem;
                color: #475569;
            }
            
            /* Welcome section */
            .welcome-section {
                background: #f8fafc;
                padding: 2rem;
                border-radius: 12px;
                margin-bottom: 2rem;
                box-shadow: 0 2px 4px rgba(0, 0, 0, 0.05);
            }
            
            .welcome-text {
                font-size: 1.1rem;
                line-height: 1.6;
                color: #1e293b;
                text-align: center;
            }
            
            /* Control buttons */
            .control-button {
                background: #ef4444 !important;
                color: white !important;
                padding: 0.6rem !important;
                border-radius: 8px !important;
                font-size: 0.9rem !important;
                font-weight: 500 !important;
            }
            
            .control-button:hover {
                background: #dc2626 !important;
            }
            
            /* Document section */
            .document-info {
                background: #f8fafc;
                padding: 1rem;
                border-radius: 8px;
                margin-bottom: 0.5rem;
                border: 1px solid #e2e8f0;
            }
            
            /* Sidebar improvements */
            .css-1d391kg {
                padding: 2rem 1rem;
            }
            
            .sidebar-title {
                font-size: 1.2rem;
                font-weight: 600;
                margin-bottom: 1rem;
                color: #1e293b;
            }
        </style>
    """, unsafe_allow_html=True)

def initialize_session_state():
    """Initialize session state only on first load"""
    # Use a flag to track if initialization has been done
    if "initialized" not in st.session_state:
        st.session_state.chats = {}
        st.session_state.current_chat = None
        st.session_state.processed = False
        st.session_state.initialized = True

def create_new_chat():
    """Create new chat with minimal checks"""
    # Since we know chats exists after initialization, we can directly create new chat
    chat_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    st.session_state.chats[chat_id] = {
        "title": "Cu·ªôc h·ªôi tho·∫°i m·ªõi",
        "messages": []
    }
    return chat_id
def render_header():
    st.markdown("""
        <div class="header">
            <div class="app-title">üö¶ Tr·ª£ L√Ω Ph√°p Lu·∫≠t Giao Th√¥ng</div>
            <div class="app-subtitle">Tra c·ª©u v√† t∆∞ v·∫•n lu·∫≠t giao th√¥ng th√¥ng minh</div>
        </div>
    """, unsafe_allow_html=True)

def main():
    setup_styles()
    
    # Initialize components
    doc_processor = DocumentProcessor()
    doc_manager = DocumentManager(doc_processor)
    traffic_assistant = TrafficLawAssistant()
    initialize_session_state()
    
    # Render header
    render_header()

    # Sidebar
    with st.sidebar:
        st.markdown('<div class="sidebar-title">üìö T√†i Li·ªáu Ph√°p Lu·∫≠t</div>', unsafe_allow_html=True)

        # Document processing section
        if not st.session_state.get("processed", False):
            with st.spinner("ƒêang t·∫£i v√† x·ª≠ l√Ω t√†i li·ªáu..."):
                if doc_manager.load_files_from_data_directory():
                    all_texts, metadata_list = doc_manager.get_processed_data()
                    vectorstore = FAISS.from_documents(all_texts, doc_processor.embeddings)
                    st.session_state.chain = traffic_assistant.setup_chain(vectorstore)
                    st.session_state.document_metadata = metadata_list
                    st.session_state.processed = True

        # Document display
        if "document_metadata" in st.session_state:
            with st.expander("üìÅ T√†i Li·ªáu ƒê√£ X·ª≠ L√Ω", expanded=True):
                for meta in st.session_state.document_metadata:
                    st.markdown(f'''
                        <div class="document-info">
                            <div style="font-weight: 500;">üìÑ {meta.title}</div>
                            <div>üìã Lo·∫°i: {meta.file_type}</div>
                            <div>üìë S·ªë trang: {meta.page_count}</div>
                            <div>üìÖ Ng√†y x·ª≠ l√Ω: {meta.upload_date.strftime('%Y-%m-%d %H:%M')}</div>
                        </div>
                    ''', unsafe_allow_html=True)

        # Chat history section
        st.divider()
        st.markdown('<div class="sidebar-title">üí¨ L·ªãch s·ª≠ chat</div>', unsafe_allow_html=True)
        
        # New chat button
        if st.button("‚ûï Cu·ªôc h·ªôi tho·∫°i m·ªõi", key="new_chat", use_container_width=True):
            st.session_state.current_chat = create_new_chat()
            st.rerun()

        # Display chat history
        for chat_id, chat_data in reversed(st.session_state.chats.items()):
            if st.button(
                f"üí¨ {chat_data['title'][:30]}...",
                key=f"chat_{chat_id}",
                use_container_width=True
            ):
                st.session_state.current_chat = chat_id
                st.rerun()

        # Control buttons
        st.divider()
        cols = st.columns(2)
        with cols[0]:
            if st.button("üóëÔ∏è X√≥a L·ªãch S·ª≠", use_container_width=True, key="clear_history"):
                st.session_state.chats = {}
                st.session_state.current_chat = None
                st.success("ƒê√£ x√≥a l·ªãch s·ª≠ chat!")
                st.rerun()

        with cols[1]:
            if st.button("üîÑ T·∫£i L·∫°i", use_container_width=True, key="reload_docs"):
                if "document_metadata" in st.session_state:
                    del st.session_state.document_metadata
                if "chain" in st.session_state:
                    del st.session_state.chain
                if hasattr(doc_manager, 'clear_data'):
                    doc_manager.clear_data()
                st.session_state.processed = False
                st.success("ƒêang t·∫£i l·∫°i t√†i li·ªáu...")
                st.rerun()

    # Chat interface remains the same
    if st.session_state.get("processed", False):
        # Initialize current chat if needed
        if not st.session_state.current_chat:
            st.session_state.current_chat = create_new_chat()

        current_chat = st.session_state.chats[st.session_state.current_chat]
        
        # Render search interface with suggestions
        render_search_interface()

        # Display messages for current chat
        for message in current_chat["messages"]:
            with st.chat_message(message["role"]):
                st.markdown(
                    f"<div class='chat-message {'user-message' if message['role'] == 'user' else 'assistant-message'}'>"
                    f"{message['content']}</div>",
                    unsafe_allow_html=True
                )

        if prompt := st.chat_input("H·ªèi v·ªÅ n·ªôi dung t√†i li·ªáu..."):
            # Update chat title with first user message if it's new
            if len(current_chat["messages"]) == 0:
                current_chat["title"] = prompt[:50]

            # Add user message
            current_chat["messages"].append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(
                    f"<div class='chat-message user-message'>{prompt}</div>",
                    unsafe_allow_html=True
                )

            # Process and display assistant response
            with st.chat_message("assistant"):
                with st.spinner("ü§î ƒêang ph√¢n t√≠ch..."):
                    response = st.session_state.chain({"question": prompt})
                    answer = response["answer"]
                    enhanced_answer = traffic_assistant.enhance_response(answer)

                    referenced_files = set()
                    for doc in response.get("source_documents", []):
                        file_path = doc.metadata.get('source', '')
                        if file_path:
                            file_name = file_path.split('/')[-1]
                            referenced_files.add(file_name)

                    sources = "\n\n**Tham kh·∫£o:**"
                    for file_name in referenced_files:
                        sources += f"\n<div class='source-citation'>üìÑ {file_name}</div>"

                    enhanced_answer = enhanced_answer.replace('**', '<strong>', 1)
                    enhanced_answer = enhanced_answer.replace('**', '</strong>', 1)

                    full_response = f"<div class='chat-message assistant-message'><p>{enhanced_answer}</p>{sources}</div>"
                    st.markdown(full_response, unsafe_allow_html=True)

                    # Add assistant response to chat history
                    current_chat["messages"].append({
                        "role": "assistant",
                        "content": f"{enhanced_answer}{sources}"
                    })
    else:
        st.info("üëÜ H√£y t·∫£i t√†i li·ªáu l√™n ƒë·ªÉ b·∫Øt ƒë·∫ßu.")

def custom_css():
    st.markdown("""
        <style>
        .stButton button {
            text-align: left;
            background-color: transparent;
            border: none;
            padding: 10px;
            line-height: 1.2;
        }
        .stButton button:hover {
            background-color: #f0f2f6;
            border-radius: 5px;
        }
        .source-citation {
            margin-top: 5px;
            color: #666;
            font-size: 0.9em;
        }
        .welcome-section {
            padding: 1rem;
            border-radius: 0.5rem;
            background: #f8f9fa;
            margin-bottom: 1rem;
        }
        .welcome-text {
            font-size: 1.1rem;
            line-height: 1.5;
            color: #1e1e1e;
        }
        </style>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    custom_css()
    main()